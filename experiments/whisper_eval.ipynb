{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1RGHdxTN0JY"
      },
      "source": [
        "IMPORT MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6lboXzQN2BS",
        "outputId": "b9c81ab0-10da-458f-9dc8-7c46a058b066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-8uopzjuo\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-8uopzjuo\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.4.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.0)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=73f4f9fac3108609297058b0c1be24564695aea3d5dd5b6698ef73715fc27e31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vs2umea8/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: rapidfuzz, ffmpeg-python, jiwer, openai-whisper\n",
            "Successfully installed ffmpeg-python-0.2.0 jiwer-4.0.0 openai-whisper-20250625 rapidfuzz-3.14.3\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git jiwer librosa soundfile ffmpeg-python numpy\n",
        "\n",
        "import whisper\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from jiwer import wer, cer\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZw8XzyVN82H"
      },
      "source": [
        "**VIDIO 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYEwTkYkN43q",
        "outputId": "95e1da7c-1a48-46b9-ed22-24621425e2ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:24<00:00, 62.3MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Text: Can you share any specific challenges you faced while working on certification and how you overcome them? Ah, okay actually, for the challenges, there are some challenges when I took the certifications, especially for the project submission that I already working with it, the first one is actually to meet the specific accuracy or validation loss for the evaluation matrix, and actually that just need to take some trial and error with different architecture, for example, we can try to add more layer, more neurons, changes the neurons, or even I also apply the dropout layer, so it really helps with the validation loss to become more lower, and I think that's one of the biggest challenges that I have while working on these certifications. Okay.\n",
            "WER: 0.1308\n",
            "CER: 0.0718\n",
            "Accuracy (%): 86.92\n"
          ]
        }
      ],
      "source": [
        "video_file = \"/content/interview_question_1.mp4\"\n",
        "audio_file = \"audio_1.wav\"\n",
        "os.system(f\"ffmpeg -i {video_file} -ar 16000 -ac 1 -vn {audio_file} -y\")\n",
        "\n",
        "# 3. Ground Truth\n",
        "ground_truth = \"\"\"Can you share any specific challenges you face while working on certification and how you overcome them? Ah, okay. Actually, for the challenges, there are some challenges when I took the certifications, especially for the projects I mentioned that I already working with it. The first one is actually to meet the specific accuracy or validation loss right for the evaluation matrix.\n",
        "And yeah, actually, that's just need to take some trial and error with different architecture. For example, we can try to add more layer, more neurons, changes the neurons, or even I also apply the dropout layer. So yeah, it really helps with the validation loss to become more lower, right? And yeah, I think that's one of the biggest challenges that I have while working on these certifications.\n",
        "\"\"\"\n",
        "\n",
        "# 4. Pilih Model Whisper\n",
        "model_name = \"medium\"  # bisa diganti tiny/base/small/medium/large-v2\n",
        "model = whisper.load_model(model_name)\n",
        "\n",
        "# 5. Transcribe & Evaluasi\n",
        "result = model.transcribe(audio_file, language=\"en\", fp16=False)\n",
        "pred_text = result[\"text\"].strip()\n",
        "\n",
        "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
        "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
        "accuracy = max(0, (1 - wer_score) * 100)\n",
        "\n",
        "print(\"Predicted Text:\", pred_text)\n",
        "print(\"WER:\", round(wer_score, 4))\n",
        "print(\"CER:\", round(cer_score, 4))\n",
        "print(\"Accuracy (%):\", round(accuracy, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe2J-_yLN-vx"
      },
      "source": [
        "**VIDIO 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JOQEw-YN7rg",
        "outputId": "a0ba6b56-de79-49d8-8439-b75bd4512ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Text: Can you describe your experience with transfer learning and time short flow? How do you benefit from projects? Ah, okay. About transfer learning is actually we use existing train model from time short flow for example like VGC 16, VGC 19, right? Especially for some cases that we need to use deep learning using Keras applications. For example like image classification, we can use transfer learning models which is already train model with exceptionally high accuracy, high performance. Even though it's train with different data sets, but it really helps to improve our model performance, model accuracy, model loss. For example like mobile net, VGG 19, VGG 16, efficient net, it will help to improve our models comparing to the one if we use traditional CNN model. CNN model with the convolutional 2D max pooling. It's quite good actually to use transfer learning. It really helps with our model performance to improve our model performance.\n",
            "WER: 0.3007\n",
            "CER: 0.0769\n",
            "Accuracy (%): 69.93\n"
          ]
        }
      ],
      "source": [
        "\n",
        "video_file = \"/content/interview_question_2.webm\"\n",
        "audio_file = \"audio_2.wav\"\n",
        "os.system(f\"ffmpeg -i {video_file} -ar 16000 -ac 1 -vn {audio_file} -y\")\n",
        "\n",
        "# 3. Ground Truth\n",
        "ground_truth = \"\"\"Can you describe your experience with transfer learning and TensorFlow? How do you benefit from the projects? About transfer learning, we use existing trained models from TensorFlow, for example, like VGG-16, VGG-19, right? Especially for some cases that we need to use deep learning using Keras applications, for example, like image classification, we can use transfer learning models, which is already a trained model with exceptionally high accuracy, high performance. Even though it's trained with different datasets, but it really helps to improve our model performance, model accuracy, model loss. For example, like MobileNet, VGG-19, VGG-16, EfficientNet, it will help to improve our models comparing to the one if we use a traditional CNN model.\n",
        "CNN model with the convolutional 2D, max pooling, and it's quite good actually to use transfer learning. It really helps with our model performance, to improve our model performance.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# 4. Pilih Model Whisper\n",
        "model_name = \"medium\"  # bisa diganti tiny/base/small/medium/large-v2\n",
        "model = whisper.load_model(model_name)\n",
        "\n",
        "# 5. Transcribe & Evaluasi\n",
        "result = model.transcribe(audio_file, language=\"en\", fp16=False)\n",
        "pred_text = result[\"text\"].strip()\n",
        "\n",
        "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
        "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
        "accuracy = max(0, (1 - wer_score) * 100)\n",
        "\n",
        "print(\"Predicted Text:\", pred_text)\n",
        "print(\"WER:\", round(wer_score, 4))\n",
        "print(\"CER:\", round(cer_score, 4))\n",
        "print(\"Accuracy (%):\", round(accuracy, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWu6NyyMOCuj"
      },
      "source": [
        "**VIDIO 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ne14G46OFBJ",
        "outputId": "a10a0b77-a7ba-4560-83f5-6110e33a5d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Text: Wait, what is this? Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency. Hmm, complex TensorFlow model you have built and steps you took to ensure its accuracy. I will take one of my previous project that I used. I also used Keras TensorFlow model. It is about cellular disease prediction. I used this model for my undergraduate thesis for my script C. I used this model. It is quite challenging even though it has achieved high accuracy with some dense layer, with some throughout layer, and trial and error with the callback function, with the neurons. But the problem is the dataset is not balanced. It has the in-balance class datasets. The approach that I used is just to use the technique called smooth and synthetic oversampling technique with edited nearest neighbor. Basically it is just oversampling and undersampling the datasets. It helps with the accuracy.\n",
            "WER: 0.2038\n",
            "CER: 0.0814\n",
            "Accuracy (%): 79.62\n"
          ]
        }
      ],
      "source": [
        "\n",
        "video_file = \"/content/interview_question_3.webm\"\n",
        "audio_file = \"audio_3.wav\"\n",
        "os.system(f\"ffmpeg -i {video_file} -ar 16000 -ac 1 -vn {audio_file} -y\")\n",
        "\n",
        "# 3. Ground Truth\n",
        "ground_truth = \"\"\"Wait, what is this? Describe a complex TensorFlow model you've built and the steps you took to ensure its accuracy and efficiency. Hmm, a complex TensorFlow model you've built and the steps you took to ensure its accuracy. Okay, I will take one of my previous project that I used.\n",
        "I also used Keras TensorFlow model. It is about celiac disease prediction. This is also I used the research project for my undergraduate thesis, for my script C. I used this model.\n",
        "It's quite challenging even though it's achieved high accuracy with some dense layer, with some drawout layer, and trial and error also with the callback function, with the neurons. But the problem is the dataset is not balanced. It has the imbalanced class datasets.\n",
        "The approach that I used is just to use the technique called smooth N, synthetic oversampling technique, with edited nearest neighbor. Basically, it's just oversampling and undersampling the datasets. It helps with the accuracy.\n",
        "\"\"\"\n",
        "\n",
        "# 4. Pilih Model Whisper\n",
        "model_name = \"medium\"  # bisa diganti tiny/base/small/medium/large-v2\n",
        "model = whisper.load_model(model_name)\n",
        "\n",
        "# 5. Transcribe & Evaluasi\n",
        "result = model.transcribe(audio_file, language=\"en\", fp16=False)\n",
        "pred_text = result[\"text\"].strip()\n",
        "\n",
        "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
        "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
        "accuracy = max(0, (1 - wer_score) * 100)\n",
        "\n",
        "print(\"Predicted Text:\", pred_text)\n",
        "print(\"WER:\", round(wer_score, 4))\n",
        "print(\"CER:\", round(cer_score, 4))\n",
        "print(\"Accuracy (%):\", round(accuracy, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DciOBsrPOHk7"
      },
      "source": [
        "**VIDIO 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN7nSkkIOJLc",
        "outputId": "9343e342-55fc-4d44-a42f-baf0b2ced6e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Text: Explain how you implement dropout in test server model and test on training. Previously, I also have implemented the dropout layer also in the project solution within this certification. We can just add the dropout layer, for example, if I'm not mistaken, I have used this dropout layer in the one that the case is image classification, German traffic something if I'm not wrong. I've used this dropout layer in the middle of the layer. So there's a flattened layer, right? Not flattened, the convolutional layer and then the flattened layer. And I use that dropout layer, which is I use with the rate of 0.2 or 0.5 if I'm not wrong. And then the dense layer and the last the output layer, right? The effect is it will really helps to improve our accuracy and lower our validation loss by turning off some of the previous layer. For example, like we have dense layer 64 and the next layer, we implement the dropout layer with the rate of 0.5 and it will turn off randomly each epoch of the previous dense layer.\n",
            "WER: 0.2616\n",
            "CER: 0.1191\n",
            "Accuracy (%): 73.84\n"
          ]
        }
      ],
      "source": [
        "\n",
        "video_file = \"/content/interview_question_4.webm\"\n",
        "audio_file = \"audio_4.wav\"\n",
        "os.system(f\"ffmpeg -i {video_file} -ar 16000 -ac 1 -vn {audio_file} -y\")\n",
        "\n",
        "# 3. Ground Truth\n",
        "ground_truth = \"\"\"Explain how to implement dropout in TensorFlow model and the effect it has on training. Previously, I also have implemented the dropout layer, also in the project solution, within these certifications. We can just add the dropout layer, for example, if I'm not mistaken, I have used this dropout layer in the image classifications, German traffic something, if I'm not wrong.\n",
        "I have used this dropout layer in the middle of the layer, so there is a flattened layer, not flattened, the convolutional layer, another flattened layer, and I used that dropout layer, which I used with the rate of 0.2 or 0.5, if I'm not wrong, and then the dense layer and the output layer. The effect is, it will help to improve our accuracy and lower our validation loss by turning off some of the previous layers. For example, we have dense layer 64, and the next layer, we implement the dropout layer with the rate of 0.5, and it will turn off randomly each epoch of the previous dense layer.\n",
        "\"\"\"\n",
        "\n",
        "# 4. Pilih Model Whisper\n",
        "model_name = \"medium\"  # bisa diganti tiny/base/small/medium/large-v2\n",
        "model = whisper.load_model(model_name)\n",
        "\n",
        "# 5. Transcribe & Evaluasi\n",
        "result = model.transcribe(audio_file, language=\"en\", fp16=False)\n",
        "pred_text = result[\"text\"].strip()\n",
        "\n",
        "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
        "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
        "accuracy = max(0, (1 - wer_score) * 100)\n",
        "\n",
        "print(\"Predicted Text:\", pred_text)\n",
        "print(\"WER:\", round(wer_score, 4))\n",
        "print(\"CER:\", round(cer_score, 4))\n",
        "print(\"Accuracy (%):\", round(accuracy, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BerXDa-HOLiy"
      },
      "source": [
        "VIDIO 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "honpjhbkONIW",
        "outputId": "6d68539e-6e92-46de-cf46-21127d1f2979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Text: Describe the process of building more confusing words for image as fiction. Okay, the CNN run, right? So, at the first time, of course, we need to make sure there are split, the image folder is split for each class. And then we can use Keras per Processing, if I'm not mistaken, image dataset from directory to split the training and validation dataset. Of course, we can use another set, which is the test dataset. Okay, the next one, we can just, maybe we need to implement also the image data augmentation tool to make our dataset more veritable. For example, we can rotate, we can zoom it, we can crop it. And the last thing, of course, we can build our model with the conventional 2D, specify the filters, the kernel size, the activation of course, the input shape for the first layer. And then we can apply the MaxPulling2D and the next layer, we can just use Conversion 2D, MaxPulling and whatever it is. And after that, we apply the Flatten layer and Dropout layer if you want. And the last thing, don't forget to use the Dance Layer for the output.\n",
            "WER: 0.1959\n",
            "CER: 0.1197\n",
            "Accuracy (%): 80.41\n"
          ]
        }
      ],
      "source": [
        "\n",
        "video_file = \"/content/interview_question_5.webm\"\n",
        "audio_file = \"audio_5.wav\"\n",
        "os.system(f\"ffmpeg -i {video_file} -ar 16000 -ac 1 -vn {audio_file} -y\")\n",
        "\n",
        "# 3. Ground Truth\n",
        "ground_truth = \"\"\"Describe the process of building or configuring your image folder for image restriction. Okay, the CNN one, right? So, at the first time, of course, we need to make sure the image folder is split for each class. And then we can use Keras preprocessing, if I'm not mistaken, image dataset from directory to split the training and the validation dataset.\n",
        "Of course, we can use another set, which is the test dataset.  Okay, the next one, we can just, maybe we need to implement also the image augmentation, data image augmentation to make our dataset more verity. For example, we can rotate, we can zoom it, we can crop it. And the last thing, of course, we can build our chain model with the convolutional 2D, specify the filters, the kernel size, the error activation, of course, the input shape for the first layer. And then we can apply the mesh pooling 2D.  And the next layer, we can just use convolutional 2D, mesh pooling, and whatever it is.\n",
        "And after that, we apply the flatten layer and dropout layer, if you want. And the last thing, don't forget to use the dense layer, right, for the output.\"\"\"\n",
        "\n",
        "# 4. Pilih Model Whisper\n",
        "model_name = \"medium\"  # bisa diganti tiny/base/small/medium/large-v2\n",
        "model = whisper.load_model(model_name)\n",
        "\n",
        "# 5. Transcribe & Evaluasi\n",
        "result = model.transcribe(audio_file, language=\"en\", fp16=False)\n",
        "pred_text = result[\"text\"].strip()\n",
        "\n",
        "wer_score = wer(ground_truth.lower(), pred_text.lower())\n",
        "cer_score = cer(ground_truth.lower(), pred_text.lower())\n",
        "accuracy = max(0, (1 - wer_score) * 100)\n",
        "\n",
        "print(\"Predicted Text:\", pred_text)\n",
        "print(\"WER:\", round(wer_score, 4))\n",
        "print(\"CER:\", round(cer_score, 4))\n",
        "print(\"Accuracy (%):\", round(accuracy, 2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GPU baru",
      "language": "python",
      "name": "gpu-baru"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
