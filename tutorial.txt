ai-interview-assessment/
â”œâ”€ app/
â”‚  â”œâ”€ app.py                      # Entry Streamlit (UI)
â”‚  â”œâ”€ components/                 # Komponen UI terpisah
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â”œâ”€ inputs.py                # select QID, input link/upload
â”‚  â”‚  â”œâ”€ results.py               # panel JSON HR, download button, tables
â”‚  â”‚  â””â”€ progress.py              # spinners, progress bars
â”‚  â”œâ”€ pages/                      # (opsional) halaman tambahan streamlit
â”‚  â”‚  â”œâ”€ 1_Evaluation.ipynb       # halaman edukasi (notebook) â€“ optional
â”‚  â”‚  â””â”€ 2_Docs.md                # dokumentasi singkat
â”‚  â””â”€ __init__.py
â”‚
â”œâ”€ core/                          # Business logic (tanpa UI)
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ config.py                   # baca config.yaml & env vars
â”‚  â”œâ”€ question_bank.py            # load YAML/JSON pertanyaan & ideal answers
â”‚  â”œâ”€ downloader.py               # fetch video dari URL (yt-dlp, gdown, direct)
â”‚  â”œâ”€ media.py                    # ekstraksi audio, normalisasi (ffmpeg/moviepy)
â”‚  â”œâ”€ stt.py                      # Speech-to-Text (Whisper / faster-whisper)
â”‚  â”œâ”€ nlp_preprocess.py           # cleaning, stopword, slang dict, stemming
â”‚  â”œâ”€ language_router.py          # deteksi bahasa (fastText/langdetect/whisper)
â”‚  â”œâ”€ similarity.py               # Sentence-BERT similarity
â”‚  â”œâ”€ keywords.py                 # keyword coverage (must/nice)
â”‚  â”œâ”€ structure.py                # skor struktur jawaban (intro-body-closing)
â”‚  â”œâ”€ confidence.py               # gabungan ASR_conf, Lang_conf, Agree_conf, Len_conf
â”‚  â”œâ”€ evaluator.py                # hitung PerformanceScore + ConfidenceScore
â”‚  â”œâ”€ serializer.py               # compose output JSON untuk HR
â”‚  â””â”€ utils.py                    # helper umum (timer, io, text normalize)
â”‚
â”œâ”€ models/                        # cache/model artefacts
â”‚  â”œâ”€ README.md
â”‚  â””â”€ (auto cached)               # SBERT, fastText lid.176.bin, dll.
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ question_bank.yaml          # definisi QID, ideal answer, keywords, weight
â”‚  â”œâ”€ slang_dict.json             # kamus normalisasi (opsional)
â”‚  â”œâ”€ stopwords_id.txt            # (opsional, jika custom)
â”‚  â””â”€ samples/                    # contoh video & GT untuk demo/test
â”‚
â”œâ”€ experiments/                   # notebook eksplorasi (bebas)
â”‚  â”œâ”€ 01_whisper_eval.ipynb
â”‚  â”œâ”€ 02_similarity_calibration.ipynb
â”‚  â””â”€ 03_streamlit_prototype.ipynb
â”‚
â”œâ”€ evaluation/                    # skrip evaluasi akurasi STT
â”‚  â”œâ”€ compute_wer.py              # batch WER/CER â†’ CSV + ringkasan
â”‚  â””â”€ datasets/
â”‚     â”œâ”€ audio/                   # *.wav
â”‚     â””â”€ gt/                      # *.txt (ground truth)
â”‚
â”œâ”€ tmp/                           # artefak runtime (gitignore)
â”‚  â”œâ”€ videos/                     # unduhan video mentah
â”‚  â”œâ”€ audio/                      # wav 16k mono
â”‚  â””â”€ transcripts/                # txt/srt/json segmen
â”‚
â”œâ”€ logs/
â”‚  â””â”€ app.log                     # logging pipeline
â”‚
â”œâ”€ tests/
â”‚  â”œâ”€ test_downloader.py
â”‚  â”œâ”€ test_stt.py
â”‚  â”œâ”€ test_evaluator.py
â”‚  â””â”€ test_confidence.py
â”‚
â”œâ”€ config.yaml                    # konfigurasi global (paths, model size, thresholds)
â”œâ”€ requirements.txt               # dependensi
â”œâ”€ .env.example                   # ENV (MODEL_SIZE, CUDA, API KEYS jika perlu)
â”œâ”€ .gitignore
â””â”€ README.md


## Isi ringkas file penting

### `requirements.txt`

```
streamlit
moviepy
ffmpeg-python
yt-dlp
requests
whisper
faster-whisper
sentence-transformers
langdetect
pyyaml
pandas
numpy
jiwer
Sastrawi
```

### `config.yaml` (contoh)

```yaml
app:
  title: "AI Interview Assessment"
  max_upload_mb: 200
paths:
  tmp_videos: "tmp/videos"
  tmp_audio: "tmp/audio"
  tmp_transcripts: "tmp/transcripts"
models:
  whisper_backend: "whisper"          # whisper | faster-whisper
  whisper_size: "base"                 # tiny/base/small/medium
  sbert_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  langdetect: "fasttext"               # fasttext | langdetect | whisper
scoring:
  pass_threshold: 0.70
  weights:
    similarity: 0.55
    keyword_must: 0.30
    keyword_nice: 0.10
    structure: 0.05
confidence:
  asr_weight: 0.7
  lang_weight: 0.3
  min_len_tokens: 120
```

### `data/question_bank.yaml` (sketsa)

```yaml
- qid: Q01
  question_text:
    id: "Ceritakan tentang dirimu dan pengalaman paling relevan."
    en: "Tell me about yourself and most relevant experience."
  languages_supported: ["id","en"]
  answers:
    id:
      ideal: "Perkenalan singkat, pengalaman relevan, pencapaian terukur, kaitkan ke role, motivasi."
      keywords:
        must: ["pengalaman","relevan","pencapaian","motivasi"]
        nice: ["angka","kolaborasi"]
    en:
      ideal: "Brief intro, relevant experience, measurable achievements, link to role, motivation."
      keywords:
        must: ["experience","relevant","achievement","motivation"]
        nice: ["metrics","collaboration"]
  weights:
    similarity: 0.55
    keyword_must: 0.30
    keyword_nice: 0.10
    structure: 0.05
  pass_threshold: 0.70
```

---

## Skeleton inti modul

### `app/app.py` (UI Streamlit yang memanggil core)

```python
import io, json, datetime
import streamlit as st
from core.config import load_config
from core.question_bank import load_qbank
from core.downloader import fetch_video_to_local
from core.media import extract_wav16k
from core.stt import transcribe
from core.evaluator import evaluate_answer
from core.serializer import compose_hr_json

st.set_page_config(page_title="AI Interview Assessment", layout="wide")
cfg = load_config()

qbank = load_qbank("data/question_bank.yaml")
qids = [q["qid"] for q in qbank]

st.sidebar.header("Pertanyaan")
sel_qid = st.sidebar.selectbox("Pilih QID", qids, index=0)
qspec = next(q for q in qbank if q["qid"] == sel_qid)
st.sidebar.caption(qspec["question_text"].get("id",""))

st.title(cfg["app"]["title"])
tab1, tab2 = st.tabs(["ðŸŽ¬ Link Video", "ðŸ“¤ Upload File"])

video_path, source_url = None, None

with tab1:
    url = st.text_input("Tempel link video:", "")
    if st.button("Proses dari Link"):
        with st.spinner("Mengunduh video..."):
            video_path = fetch_video_to_local(url, cfg)
            source_url = url

with tab2:
    f = st.file_uploader("Unggah video (mp4/mov/webm)", type=["mp4","mov","webm"])
    if st.button("Proses dari Upload"):
        if not f: st.error("Belum pilih file"); st.stop()
        from pathlib import Path
        p = Path(cfg["paths"]["tmp_videos"]) / f.name
        p.parent.mkdir(parents=True, exist_ok=True)
        p.write_bytes(f.read())
        video_path = p

if video_path:
    with st.spinner("Ekstrak audio & transkrip..."):
        wav = extract_wav16k(video_path, cfg)
        text, segments, meta = transcribe(wav, cfg)

    with st.spinner("Skoring..."):
        result = evaluate_answer(text, qspec, meta, cfg)

    out = compose_hr_json(qspec, text, result, meta, source_url, video_path)
    st.subheader("Hasil (JSON untuk HR)")
    st.json(out)

    buf = io.BytesIO(json.dumps(out, ensure_ascii=False, indent=2).encode("utf-8"))
    st.download_button("Download JSON", data=buf, file_name=f"{qspec['qid']}_result.json", mime="application/json")
```

### `core/config.py`

```python
import yaml
def load_config(path="config.yaml"):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)
```

### `core/downloader.py`

```python
from pathlib import Path
import os, requests

def _download_direct(url: str, outpath: Path):
    outpath.parent.mkdir(parents=True, exist_ok=True)
    with requests.get(url, stream=True, timeout=60) as r:
        r.raise_for_status()
        with open(outpath, "wb") as f:
            for c in r.iter_content(chunk_size=1<<20):
                if c: f.write(c)
    return outpath

def _download_ytdlp(url: str, outdir: Path) -> Path:
    import yt_dlp
    outdir.mkdir(parents=True, exist_ok=True)
    outtpl = str(outdir / "%(title).80s_%(id)s.%(ext)s")
    ydl_opts = {"quiet": True, "outtmpl": outtpl, "format": "mp4+bestaudio/best", "merge_output_format":"mp4", "noplaylist": True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=True)
        path = ydl.prepare_filename(info)
    base = os.path.splitext(path)[0] + ".mp4"
    return Path(base if os.path.exists(base) else path)

def fetch_video_to_local(url: str, cfg) -> Path:
    u = url.lower()
    outdir = Path(cfg["paths"]["tmp_videos"])
    if any(k in u for k in ["youtube.com","youtu.be","tiktok.com","x.com"]):
        return _download_ytdlp(url, outdir)
    if "dropbox.com" in u and "?dl=0" in url:
        url = url.replace("?dl=0","?dl=1")
    return _download_direct(url, outdir / "remote_video.mp4")
```

### `core/media.py`

```python
from moviepy.editor import VideoFileClip
from pathlib import Path

def extract_wav16k(video_path: Path, cfg) -> Path:
    outdir = Path(cfg["paths"]["tmp_audio"]); outdir.mkdir(parents=True, exist_ok=True)
    out = outdir / (video_path.stem + ".16k.wav")
    clip = VideoFileClip(video_path.as_posix())
    clip.audio.write_audiofile(out.as_posix(), fps=16000, nbytes=2, codec="pcm_s16le", ffmpeg_params=["-ac","1"])
    clip.close()
    return out
```

### `core/stt.py`

```python
def _whisper(wav_path, cfg):
    import whisper
    model = whisper.load_model(cfg["models"]["whisper_size"])
    result = model.transcribe(str(wav_path), language="id", word_timestamps=False)
    text = (result.get("text") or "").strip()
    segs = result.get("segments", [])
    if segs:
        avg_logprob = sum(s.get("avg_logprob",-1.0) for s in segs)/len(segs)
        ns_prob = sum(s.get("no_speech_prob",0.0) for s in segs)/len(segs)
        dur = float(segs[-1]["end"] - segs[0]["start"])
    else:
        avg_logprob, ns_prob, dur = -2.0, 1.0, 0.0
    return text, segs, {"avg_logprob": avg_logprob, "no_speech_prob": ns_prob, "duration_sec": dur}

def transcribe(wav_path, cfg):
    backend = cfg["models"]["whisper_backend"]
    return _whisper(wav_path, cfg)
```

### `core/evaluator.py`

```python
from core.language_router import detect_language
from core.similarity import sbert_similarity
from core.keywords import coverage
from core.structure import structure_score
from core.confidence import confidence_score

def evaluate_answer(transcript_text: str, qspec: dict, whisper_meta: dict, cfg: dict):
    lang_det = detect_language(transcript_text)
    lang = lang_det["lang"] if lang_det["lang"] in qspec["languages_supported"] else qspec["languages_supported"][0]

    ideal = qspec["answers"][lang]["ideal"]
    must = qspec["answers"][lang]["keywords"]["must"]
    nice = qspec["answers"][lang]["keywords"]["nice"]

    sim = sbert_similarity(transcript_text, ideal, cfg["models"]["sbert_name"])
    must_hits, must_cov = coverage(transcript_text, must)
    nice_hits, nice_cov = coverage(transcript_text, nice)
    struct = structure_score(transcript_text)

    w = qspec["weights"]
    perf = w["similarity"]*sim + w["keyword_must"]*must_cov + w["keyword_nice"]*nice_cov + w["structure"]*struct

    conf = confidence_score(whisper_meta, lang_det, sim, must_cov, len(transcript_text.split()), cfg)

    return {
        "lang_selected": lang,
        "sim": sim,
        "keyword_must_coverage": must_cov,
        "keyword_nice_coverage": nice_cov,
        "structure": float(struct),
        "performance_score": perf,
        "confidence_score": conf,
        "hits": {"must": must_hits, "nice": nice_hits}
    }
```

### `core/confidence.py`

```python
import math

def asr_confidence(avg_logprob: float, no_speech_prob: float, aw=0.7):
    x = (avg_logprob + 2.0) / (2.0 - 0.1)
    x = max(0.0, min(1.0, x))
    y = 1.0 - no_speech_prob
    return max(0.0, min(1.0, aw*x + (1-aw)*y))

def geom_mean(vals):
    vals = [max(1e-6, min(1.0, v)) for v in vals]
    return float(math.exp(sum(math.log(v) for v in vals)/len(vals)))

def confidence_score(whisper_meta, lang_det, sim, must_cov, length_tokens, cfg):
    asr = asr_confidence(whisper_meta["avg_logprob"], whisper_meta["no_speech_prob"], cfg["confidence"]["asr_weight"])
    langc = lang_det.get("confidence", 0.6)
    agree = max(1e-6, min(1.0, (sim + must_cov)/2))
    lenf = min(1.0, length_tokens/ cfg["confidence"]["min_len_tokens"])
    return geom_mean([asr, langc, agree, lenf])
```

### `core/serializer.py`

```python
from datetime import datetime, timezone

def compose_hr_json(qspec, transcript, result, meta, source_url, video_path):
    now = datetime.now().astimezone().isoformat()
    return {
        "qid": qspec["qid"],
        "question_text": qspec["question_text"].get("id") or next(iter(qspec["question_text"].values())),
        "language_selected": result["lang_selected"],
        "asr": {
            "avg_logprob": round(meta["avg_logprob"],4),
            "no_speech_prob": round(meta["no_speech_prob"],4),
            "duration_sec": round(meta["duration_sec"],2)
        },
        "transcript": transcript,
        "scores": {
            "similarity": round(result["sim"],4),
            "keyword_must_coverage": round(result["keyword_must_coverage"],4),
            "keyword_nice_coverage": round(result["keyword_nice_coverage"],4),
            "structure": result["structure"],
            "performance_score": round(result["performance_score"],4),
            "confidence_score": round(result["confidence_score"],4)
        },
        "keyword_hits": result["hits"],
        "video_meta": {
            "source_url": source_url,
            "saved_video": str(video_path)
        },
        "timestamp": now
    }
```

---

## Cara menjalankan

```bash
# 1) buat venv lalu install deps
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 2) jalankan UI
streamlit run app/app.py
```

---

Kalau kamu mau, aku bisa menyiapkan **arsip proyek awal (folder & file kosong + skeleton di atas)** supaya kamu tinggal isi logic-nya. Mau pakai **Whisper open-source** atau **faster-whisper** sebagai default backend?
